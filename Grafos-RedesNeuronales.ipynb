{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementacion de Modelos de Redes Neuronales a Grafos.\n",
        "\n",
        "**Autor: Jorge Xavier Romero**"
      ],
      "metadata": {
        "id": "b2Di1ovHFa3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Predicción de etiquetas de nodos"
      ],
      "metadata": {
        "id": "byjBFva8Gg-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Importas generales e instalación"
      ],
      "metadata": {
        "id": "Z_2xG0K8jOg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "#   !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "#   !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "#   !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "#   !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ],
      "metadata": {
        "id": "iCuUujZzjSp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de6a353-7da0-4090-b175-ae051ace125c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-cqyfqd0d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-cqyfqd0d\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 5ff3295250715f765ac60dfd6c34e0fc7f1cd904\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.4.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=960669 sha256=a0c2ce59800c83720e28070f01ec8e384742368cd18d53b88c5c6ed3645161e4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fx3832qt/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.15)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=448c53cb5d146574b6a6bd691757a998d16424974e09766181cce9c551a7b678\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Carga del dataset"
      ],
      "metadata": {
        "id": "nXCBE-NCGt7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te recomendamos montar tu drive en colab y luego utilizar `gdown` para descargar los datos a drive o, alternativamente, descargar los datos desde [este link](https://drive.google.com/file/d/17eMe-4MFx8mmVuLgf6pcz1NVKJ9cP96u/view?usp=sharing) y luego subirlos manualmente a drive.\n",
        "\n",
        "Para cargar el dataset debes utilizar pickle para deserializar el archivo `dataset` y luego responder las preguntas que se encuentran a continuación."
      ],
      "metadata": {
        "id": "jGggFnXiYJR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJNwZlZ8JCAc",
        "outputId": "09b5d3fc-ccca-4489-924e-2b5cc028c8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 17eMe-4MFx8mmVuLgf6pcz1NVKJ9cP96u"
      ],
      "metadata": {
        "id": "sXTpMh2sG8A7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e806701-dd65-4503-b252-ce96f2c9ed41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17eMe-4MFx8mmVuLgf6pcz1NVKJ9cP96u\n",
            "To: /content/dataset\n",
            "100% 895M/895M [00:16<00:00, 54.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('drive/MyDrive//Backup/dataset', 'rb') as file:\n",
        "  data = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "aXuhz3zlCk4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.x)\n",
        "print(data.edge_index)\n",
        "print(data.y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W8UGaZDNFJS",
        "outputId": "918f9573-2456-4aba-caf9-8639106adbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(data))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqj-JB54NS_n",
        "outputId": "ceef9261-83b4-467e-88b2-1e0203a6b6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch_geometric.data.data.Data'>\n",
            "Data(\n",
            "  num_nodes_dict={\n",
            "    author=1134649,\n",
            "    field_of_study=59965,\n",
            "    institution=8740,\n",
            "    paper=736389,\n",
            "  },\n",
            "  edge_index_dict={\n",
            "    (author, affiliated_with, institution)=[2, 1043998],\n",
            "    (author, writes, paper)=[2, 7145660],\n",
            "    (paper, cites, paper)=[2, 5416271],\n",
            "    (paper, has_topic, field_of_study)=[2, 7505078],\n",
            "  },\n",
            "  x_dict={ paper=[736389, 128] },\n",
            "  node_year={ paper=[736389, 1] },\n",
            "  edge_reltype={\n",
            "    (author, affiliated_with, institution)=[1043998, 1],\n",
            "    (author, writes, paper)=[7145660, 1],\n",
            "    (paper, cites, paper)=[5416271, 1],\n",
            "    (paper, has_topic, field_of_study)=[7505078, 1],\n",
            "  },\n",
            "  y_dict={ paper=[736389, 1] }\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir el número de nodos y bordes para cada tipo\n",
        "for node_type, num in data.num_nodes_dict.items():\n",
        "    print(f\"{node_type}: {num} nodos\")\n",
        "\n",
        "for edge_type, edge_index in data.edge_index_dict.items():\n",
        "    print(f\"{edge_type}: {len(edge_index[0])} bordes\")\n",
        "\n",
        "# Imprimir algunas características y etiquetas de 'paper'\n",
        "print(\"Primeras 5 características de 'paper':\", data.x_dict['paper'][:5])\n",
        "print(\"Primeras 5 etiquetas de 'paper':\", data.y_dict['paper'][:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NezpLjdYNjmV",
        "outputId": "897f1f9e-01cb-430c-821b-7fbcf42f6d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "author: 1134649 nodos\n",
            "field_of_study: 59965 nodos\n",
            "institution: 8740 nodos\n",
            "paper: 736389 nodos\n",
            "('author', 'affiliated_with', 'institution'): 1043998 bordes\n",
            "('author', 'writes', 'paper'): 7145660 bordes\n",
            "('paper', 'cites', 'paper'): 5416271 bordes\n",
            "('paper', 'has_topic', 'field_of_study'): 7505078 bordes\n",
            "Primeras 5 características de 'paper': tensor([[-9.5379e-02,  4.0758e-02, -2.1095e-01, -6.4362e-02, -2.2594e-01,\n",
            "         -2.3230e-03, -5.5583e-01, -3.5291e-01,  4.0634e-02, -1.9523e-01,\n",
            "         -1.2195e-01, -4.9836e-01,  5.7569e-02, -1.4867e-01, -1.5447e-01,\n",
            "          2.6752e-01, -7.8571e-02,  3.0968e-01,  1.6450e-01, -1.5782e-01,\n",
            "         -2.0708e-01,  2.6488e-01, -2.7965e-01, -1.4788e-01,  1.8810e-03,\n",
            "          4.9859e-01,  2.6378e-01, -1.7021e-01, -1.5862e-01,  1.7871e-01,\n",
            "         -9.6597e-02, -1.2366e-01, -2.5665e-02, -1.0387e-01, -1.6596e-02,\n",
            "          6.8700e-04, -2.2242e-01,  5.5000e-05,  8.0517e-02,  4.9467e-01,\n",
            "          1.9211e-01, -6.4611e-02,  4.5427e-02, -1.0394e-02,  3.7437e-01,\n",
            "         -1.0860e-02,  1.7933e-01, -6.6018e-02,  3.4255e-01,  1.8213e-01,\n",
            "          5.7058e-02,  5.0556e-02,  4.3272e-02, -3.0460e-03, -2.2085e-01,\n",
            "         -9.1142e-02, -8.5992e-02, -1.2618e-01, -5.1549e-02, -6.6814e-02,\n",
            "          1.5505e-01,  7.6750e-03,  3.9405e-02, -1.0285e-01, -1.5280e-01,\n",
            "          2.5422e-01, -4.4090e-03,  1.2442e-01,  4.0386e-01,  1.9975e-02,\n",
            "         -2.1614e-01,  4.1033e-02,  5.9115e-02, -1.6157e-01,  1.5090e-02,\n",
            "         -3.8502e-01,  7.6648e-02,  1.0113e-01, -9.1230e-02, -1.3844e-01,\n",
            "         -3.2547e-01,  5.7070e-02,  6.2121e-01, -4.6403e-01, -3.6814e-02,\n",
            "          1.9199e-01, -1.7694e-01,  1.9993e-02,  9.3149e-01,  1.2869e-01,\n",
            "          3.0553e-01,  1.5680e-01,  1.9763e-01,  3.9120e-02,  8.9195e-02,\n",
            "         -1.2153e-01, -1.8509e-01,  3.3305e-01,  3.5070e-01, -1.1454e-01,\n",
            "         -2.4512e-01,  6.1302e-01, -2.2890e-01, -8.2864e-02, -4.9529e-02,\n",
            "          8.8837e-02, -7.7158e-02, -1.5035e-02,  1.0349e-01,  6.2701e-02,\n",
            "         -8.4663e-02, -1.9051e-01, -1.2160e-01,  5.0540e-01,  1.7576e-01,\n",
            "          2.2969e-01,  2.1820e-01, -1.7535e-01,  1.4639e-01, -3.7865e-01,\n",
            "         -3.2887e-01, -1.3688e-01, -7.7790e-03, -1.5763e-01, -6.9693e-02,\n",
            "          6.1569e-02, -2.7663e-02, -1.3383e-01],\n",
            "        [-1.5105e-01, -1.0731e-01, -2.2196e-01, -3.4725e-02,  1.0814e-02,\n",
            "         -9.6730e-03, -4.3432e-01, -4.6173e-01,  1.1273e-01,  9.1751e-02,\n",
            "         -2.9699e-01, -4.1827e-01,  2.8686e-02,  1.1026e-01, -2.8753e-02,\n",
            "          2.5722e-01, -7.4152e-02,  2.7809e-01,  1.3099e-01, -2.4411e-01,\n",
            "         -2.5747e-01,  2.9743e-01, -3.2160e-01, -3.8277e-02, -6.3680e-03,\n",
            "          5.8514e-01,  2.4703e-01, -2.9629e-01, -4.3533e-02,  1.3743e-01,\n",
            "         -1.8801e-01, -2.0516e-01, -3.2521e-02, -9.9050e-03, -5.1832e-02,\n",
            "         -1.7173e-01, -1.8467e-01,  1.2411e-01,  9.2062e-02,  2.3200e-01,\n",
            "         -7.8310e-03,  1.4514e-02,  1.3919e-02, -8.2304e-02,  1.9118e-02,\n",
            "         -1.2742e-02,  2.4021e-01, -2.1093e-01,  2.3455e-01,  2.8941e-01,\n",
            "          2.4316e-02,  2.9524e-02, -8.9509e-02, -1.2913e-01, -6.8262e-02,\n",
            "          1.4067e-02, -5.3470e-02, -1.6967e-01, -4.1905e-02,  5.8950e-03,\n",
            "          2.5758e-01, -7.0653e-02,  2.1210e-03,  2.7341e-02,  8.4450e-02,\n",
            "          4.7947e-01, -7.5232e-02,  1.3099e-01,  1.6647e-01, -1.1650e-01,\n",
            "         -4.4505e-02,  1.3171e-01,  3.1751e-01, -3.6165e-01, -9.4110e-03,\n",
            "         -3.0654e-01,  1.2190e-01,  5.9490e-02, -1.1370e-01, -1.7791e-02,\n",
            "         -1.0836e-01, -1.0863e-01,  4.2352e-01, -2.0963e-01,  1.3483e-01,\n",
            "          2.7548e-02, -2.2974e-01,  1.7723e-01,  9.2081e-01, -1.2434e-02,\n",
            "          2.1832e-01,  5.5737e-02,  1.9360e-01, -2.4791e-01,  1.0201e-01,\n",
            "          4.5616e-02, -1.6457e-01,  8.0162e-02,  5.3590e-03, -1.8665e-01,\n",
            "         -2.7234e-01,  7.2722e-01, -3.3823e-01, -7.0547e-02,  2.9240e-01,\n",
            "          2.3225e-01, -9.8406e-02,  1.3547e-01,  1.0190e-01,  2.2251e-02,\n",
            "         -3.1781e-01,  5.2310e-03,  1.0715e-01,  2.6708e-01,  4.5123e-01,\n",
            "          1.9420e-01,  3.3106e-01,  8.2356e-02,  4.7749e-02, -6.5133e-02,\n",
            "         -2.0889e-01, -3.6601e-01, -4.5869e-02, -2.9747e-01,  5.6755e-02,\n",
            "          3.4575e-01, -2.7737e-02, -2.1853e-01],\n",
            "        [-1.1480e-01, -1.7598e-01, -2.6056e-01,  1.1920e-02, -1.2927e-01,\n",
            "         -4.4518e-02, -3.4542e-01, -3.7826e-01, -8.8449e-02, -9.1060e-02,\n",
            "         -8.6670e-03, -5.0439e-01,  3.1037e-01,  3.2474e-02, -4.3397e-01,\n",
            "          2.3493e-01,  1.3749e-02,  4.3548e-01,  6.0240e-03, -3.0001e-01,\n",
            "         -3.0249e-01,  9.9779e-02, -3.7291e-01,  4.0447e-02, -1.0075e-01,\n",
            "          5.3613e-01,  1.9835e-01, -2.5827e-01, -6.5260e-02, -2.1997e-02,\n",
            "         -3.7900e-01,  1.0246e-01, -1.0312e-01,  2.1765e-01, -1.3423e-02,\n",
            "         -7.3044e-02, -3.8905e-02,  1.9765e-01, -1.2924e-01, -3.9720e-03,\n",
            "          1.5603e-01, -1.7679e-01,  1.7224e-01, -5.7212e-02,  3.0778e-01,\n",
            "          1.1798e-01,  1.8332e-01, -8.5390e-03,  4.4178e-01,  6.0780e-02,\n",
            "          2.1435e-01,  9.8065e-02,  1.4341e-01, -2.4626e-01,  1.1010e-01,\n",
            "          1.0491e-01, -1.1788e-01, -3.6061e-02,  9.1070e-02,  1.1456e-01,\n",
            "          3.1211e-01, -5.1275e-02,  3.3011e-02,  9.8112e-02,  1.2453e-01,\n",
            "          3.5344e-01, -1.7146e-01,  7.6597e-02,  4.0213e-01, -1.5017e-01,\n",
            "          8.8045e-02, -1.3084e-01,  4.4759e-01, -3.7581e-01, -1.2455e-01,\n",
            "         -3.2531e-01, -4.5349e-02,  3.5266e-02,  1.6170e-01, -1.6718e-01,\n",
            "         -2.5676e-01,  5.9875e-02,  4.8891e-01, -3.5932e-01, -1.6320e-01,\n",
            "          1.6383e-02,  5.4265e-02,  2.0337e-01,  9.9607e-01,  1.7659e-01,\n",
            "          2.7088e-01, -6.4194e-02,  1.8918e-01,  5.8619e-02, -9.5420e-02,\n",
            "          1.2903e-01, -2.1160e-03,  8.3209e-02,  6.0848e-02,  1.0684e-02,\n",
            "         -5.1782e-02,  8.0582e-01, -3.6783e-01,  1.8249e-02,  1.0162e-01,\n",
            "          2.4767e-01, -1.3062e-01,  2.6991e-01,  5.8048e-02, -7.4666e-02,\n",
            "         -1.9156e-01, -3.5317e-01,  1.1759e-01,  3.5742e-01,  1.9997e-01,\n",
            "          1.5694e-01,  3.6661e-01,  1.2034e-01,  4.7763e-02, -3.9451e-01,\n",
            "         -2.3878e-01,  8.0928e-02,  5.7844e-02, -2.0510e-01, -6.7513e-02,\n",
            "          1.7306e-01, -1.5644e-01, -2.7795e-01],\n",
            "        [ 4.5060e-03,  4.2368e-02, -1.7846e-01, -1.3848e-01, -1.3827e-01,\n",
            "          2.4902e-02, -4.3815e-01, -4.5013e-01,  6.6984e-02, -1.7753e-02,\n",
            "         -2.0940e-01, -4.1047e-01,  1.1203e-01, -7.6270e-02, -3.5567e-02,\n",
            "          8.8536e-02, -1.1469e-01,  4.0529e-01,  2.6142e-01, -4.7756e-02,\n",
            "         -1.9171e-01,  3.0029e-01, -4.1426e-01,  2.0460e-02, -1.0604e-01,\n",
            "          4.6297e-01,  1.6179e-01, -4.9334e-01, -2.5164e-02,  7.2559e-02,\n",
            "         -1.5987e-01,  2.5410e-02,  5.0227e-02,  4.3551e-02, -4.3574e-02,\n",
            "         -1.9938e-01, -1.2940e-01, -1.8994e-02,  4.9996e-02,  1.5060e-01,\n",
            "          2.1756e-02,  1.0889e-01,  6.6380e-03,  1.6588e-02,  1.2009e-01,\n",
            "          7.4749e-02,  2.1173e-01, -1.3221e-01,  1.9683e-01,  1.3537e-01,\n",
            "          1.3498e-01,  8.9875e-02, -3.9180e-03, -4.0440e-02, -1.4042e-01,\n",
            "         -1.2441e-02,  8.4976e-02,  9.0890e-03,  5.7914e-02,  1.0959e-01,\n",
            "          1.1441e-01, -8.7310e-03, -4.5393e-02, -1.1551e-01, -3.1140e-02,\n",
            "          4.7400e-01, -6.7748e-02,  5.6053e-02,  1.6108e-01, -2.5976e-02,\n",
            "          1.7920e-03,  2.3510e-02,  1.3910e-01, -2.4827e-01, -5.2847e-02,\n",
            "         -1.7059e-01,  1.5143e-01,  1.4015e-01, -1.5434e-01,  1.3907e-02,\n",
            "         -2.3840e-01, -3.5174e-02,  6.3886e-01, -2.6236e-02,  9.6613e-02,\n",
            "          2.0318e-01, -4.0299e-01,  1.6178e-01,  8.3281e-01,  2.3333e-01,\n",
            "          2.1771e-01,  8.3790e-03,  2.1743e-02, -9.2705e-02,  8.8821e-02,\n",
            "         -6.4246e-02, -1.7551e-01,  1.1223e-01,  6.3041e-02, -1.6185e-01,\n",
            "         -1.3967e-01,  7.9071e-01, -1.7547e-01, -2.3098e-01,  3.8524e-01,\n",
            "          1.3152e-01, -2.3191e-01,  3.8320e-03,  7.7050e-02, -4.0889e-02,\n",
            "         -3.4091e-01, -1.6830e-01,  7.6943e-02,  8.2116e-02,  4.6094e-01,\n",
            "          2.6821e-01,  2.1678e-01, -1.4472e-01,  1.1650e-01, -2.5529e-01,\n",
            "         -2.7823e-01, -8.1235e-02,  2.5197e-01, -6.6206e-02,  9.5656e-02,\n",
            "          2.8223e-01,  1.6337e-02, -2.1273e-01],\n",
            "        [-9.4474e-02, -8.0044e-02, -2.2247e-01, -1.5895e-01, -2.4708e-02,\n",
            "          4.3510e-03, -4.6367e-01, -3.4465e-01, -1.0181e-01, -1.2426e-02,\n",
            "          8.1345e-02, -2.7277e-01,  1.4478e-01, -4.6082e-02, -1.9583e-01,\n",
            "          1.1726e-01,  2.9953e-02,  5.0032e-01,  1.2524e-01, -2.3484e-01,\n",
            "         -3.7675e-01,  5.5360e-02, -3.8463e-01, -1.5179e-02, -4.2762e-02,\n",
            "          4.0231e-01,  8.8341e-02, -2.1131e-01, -1.3297e-01,  1.6232e-01,\n",
            "         -1.9156e-01, -2.8000e-04, -9.9853e-02,  7.2018e-02,  3.4592e-02,\n",
            "         -2.3570e-01, -2.3140e-01,  1.9157e-01, -2.5820e-03,  1.5757e-01,\n",
            "          2.7880e-02, -3.8813e-02,  1.5091e-01, -1.3638e-02,  4.0795e-02,\n",
            "         -7.0812e-02,  7.8765e-02, -2.7574e-02,  2.5449e-01,  5.0449e-02,\n",
            "          1.9781e-01,  4.0590e-03, -2.5995e-02, -9.5498e-02,  4.9258e-02,\n",
            "          1.2782e-01, -8.8255e-02, -3.7818e-02,  5.3668e-02,  4.0797e-02,\n",
            "          2.7498e-01, -1.4117e-01,  6.0543e-02,  1.2352e-02, -2.2791e-02,\n",
            "          3.0493e-01, -1.5411e-01,  3.0163e-02,  2.4784e-01, -2.1248e-01,\n",
            "          2.7042e-02,  3.8383e-02,  1.5624e-01, -3.3073e-01,  7.8175e-02,\n",
            "         -3.0403e-01,  1.6308e-01,  1.2953e-01, -7.2390e-03,  6.1697e-02,\n",
            "         -2.4484e-01, -8.9059e-02,  4.2778e-01, -1.4358e-01,  1.1204e-01,\n",
            "          1.2364e-01, -1.9936e-01,  2.0723e-01,  8.7478e-01,  9.9883e-02,\n",
            "          3.3547e-01, -2.0754e-02,  9.2785e-02, -1.2404e-01,  1.4173e-02,\n",
            "          2.2450e-01, -8.1779e-02, -1.9140e-03,  5.7839e-02, -1.0343e-01,\n",
            "         -1.2498e-01,  6.3017e-01, -3.3125e-01, -9.7742e-02,  1.2139e-01,\n",
            "          3.0014e-01, -5.2520e-03,  8.5460e-02,  3.8788e-02, -1.0634e-01,\n",
            "         -2.5252e-01, -2.1081e-01,  8.0070e-02,  1.5237e-01,  2.1902e-01,\n",
            "          1.8012e-01,  1.3983e-01, -6.8196e-02, -1.1792e-01, -1.3936e-01,\n",
            "         -2.2897e-01, -6.2332e-02,  1.3285e-01, -2.6912e-01,  2.4864e-02,\n",
            "          2.8878e-01, -1.8699e-01, -2.0509e-01]])\n",
            "Primeras 5 etiquetas de 'paper': tensor([[246],\n",
            "        [131],\n",
            "        [189],\n",
            "        [131],\n",
            "        [ 95]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que todos los índices de bordes son válidos\n",
        "for edge_type, edge_index in data.edge_index_dict.items():\n",
        "    src_type, _, dst_type = edge_type\n",
        "    assert edge_index[0].max() < data.num_nodes_dict[src_type]\n",
        "    assert edge_index[1].max() < data.num_nodes_dict[dst_type]\n",
        "\n",
        "# Verificar que las características y las etiquetas tienen la longitud correcta\n",
        "assert data.x_dict['paper'].shape[0] == data.num_nodes_dict['paper']\n",
        "assert data.y_dict['paper'].shape[0] == data.num_nodes_dict['paper']\n"
      ],
      "metadata": {
        "id": "wRAA3Nx8N1SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "Una vez cargado el dataset debes responder las siguientes preguntas:\n",
        "\n",
        "1. ¿De qué tipo de objeto es el dataset cargado?\n",
        "\n",
        "R. El dataset cargado es de clase torch.geometric, el mismo que corresponde a un grafo que cuenta con 4 nodos y 4 relaciones.\n",
        "\n",
        "\n",
        "2. ¿Qué tipos de nodos y aristas tiene el grafo?\n",
        "\n",
        "R. El grafo tiene los siguientes tipos de nodos y aristas:\n",
        "\n",
        "Tipos de nodos: 'author', 'field_of_study', 'institution', 'paper'\n",
        "Tipos de aristas:\n",
        "('author', 'affiliated_with', 'institution')\n",
        "('author', 'writes', 'paper')\n",
        "('paper', 'cites', 'paper')\n",
        "('paper', 'has_topic', 'field_of_study')\n",
        "\n",
        "3. ¿Cuántas features tiene cada nodo y arista?\n",
        "\n",
        "R. Cada nodo del tipo 'paper' tiene 128 features.\n",
        "\n",
        "Cada una de las aristas tiene un feature\n",
        "\n",
        "4. ¿Cuántos nodos y aristas, de cada tipo, forman el grafo?\n",
        "\n",
        "R. Número de nodos:\n",
        "'author': 1,134,649 nodos\n",
        "'field_of_study': 59,965 nodos\n",
        "'institution': 8,740 nodos\n",
        "'paper': 736,389 nodos\n",
        "\n",
        "Número de aristas:\n",
        "('author', 'affiliated_with', 'institution'): 1,043,998 aristas\n",
        "('author', 'writes', 'paper'): 7,145,660 aristas\n",
        "('paper', 'cites', 'paper'): 5,416,271 aristas\n",
        "('paper', 'has_topic', 'field_of_study'): 7,505,078 aristas\n"
      ],
      "metadata": {
        "id": "ZbbqZ_CiKZws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Filtro de datos"
      ],
      "metadata": {
        "id": "sVhYLbkJd2rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('drive/MyDrive//Backup/split', 'rb') as file:\n",
        "  data = pickle.load(file)"
      ],
      "metadata": {
        "id": "NesqXkS2FsQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solamente utilizaremos los datos que relacionan papers entre sí, para luego realizar predicciones.\n",
        "\n",
        "Crea un nuevo dataset a partir de los datos cargados que contenga:\n",
        "- En x la información de los papers de `x_dict`\n",
        "- En y la información de los papers de `y_dict`\n",
        "- En edge_index la información de las aristas con la forma `('paper', 'cites', 'paper')`"
      ],
      "metadata": {
        "id": "GsVwS14Gd9Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Obtener los datos específicos que necesitamos\n",
        "x = data.x_dict['paper']\n",
        "edge_index = data.edge_index_dict[('paper', 'cites', 'paper')]\n",
        "y = data.y_dict['paper']\n",
        "\n",
        "\n",
        "# Crear un nuevo objeto Data con los datos seleccionados\n",
        "new_data = Data(x=x, y=y, edge_index=edge_index)\n",
        "print(new_data)\n"
      ],
      "metadata": {
        "id": "u0azyWSyfRUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0ecf1f-1601-418a-f85e-5c5f5c1efcdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[736389, 128], edge_index=[2, 5416271], y=[736389, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamaño del dataset en términos de nodos y aristas\n",
        "print(\"Número de nodos en el nuevo dataset:\", new_data.num_nodes)\n",
        "print(\"Número de aristas en el nuevo dataset:\", new_data.num_edges)\n",
        "\n",
        "# Número de características por nodo (dimensión de 'x') y número de etiquetas (dimensión de 'y')\n",
        "print(\"Número de características por nodo:\", new_data.num_node_features)\n",
        "print(\"Número de etiquetas:\", new_data.y.size(0))\n",
        "\n",
        "# Comparación con el dataset original\n",
        "original_num_nodes = data.num_nodes_dict['paper']\n",
        "original_num_edges = data.edge_index_dict[('paper', 'cites', 'paper')].size(1)\n",
        "\n",
        "# Asumiendo que las características del nodo 'paper' y las etiquetas también se almacenan en el dataset original\n",
        "original_num_node_features = data.x_dict['paper'].size(1)\n",
        "original_num_labels = data.y_dict['paper'].size(0)\n",
        "\n",
        "# Comparar\n",
        "print(\"¿Mismo número de nodos que el dataset original?\", new_data.num_nodes == original_num_nodes)\n",
        "print(\"¿Mismo número de aristas que el dataset original?\", new_data.num_edges == original_num_edges)\n",
        "print(\"¿Mismo número de características por nodo que el dataset original?\", new_data.num_node_features == original_num_node_features)\n",
        "print(\"¿Mismo número de etiquetas que el dataset original?\", new_data.y.size(0) == original_num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Ju-GD30CwB",
        "outputId": "291c6088-e89b-410b-dca7-1c0b8b968ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de nodos en el nuevo dataset: 736389\n",
            "Número de aristas en el nuevo dataset: 5416271\n",
            "Número de características por nodo: 128\n",
            "Número de etiquetas: 736389\n",
            "¿Mismo número de nodos que el dataset original? True\n",
            "¿Mismo número de aristas que el dataset original? True\n",
            "¿Mismo número de características por nodo que el dataset original? True\n",
            "¿Mismo número de etiquetas que el dataset original? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamaño del dataset creado\n",
        "size_dataset = len(new_data)\n",
        "print(\"Tamaño del dataset creado:\", size_dataset)\n",
        "\n",
        "# Cantidad de nodos, aristas y etiquetas en el dataset creado\n",
        "num_nodes = new_data.num_nodes\n",
        "num_edges = new_data.num_edges\n",
        "num_labels = new_data.y.shape[0]\n",
        "print(\"Cantidad de nodos:\", num_nodes)\n",
        "print(\"Cantidad de aristas:\", num_edges)\n",
        "print(\"Cantidad de etiquetas:\", num_labels)\n",
        "\n",
        "# Comparación con el dataset original\n",
        "original_num_nodes = data.num_nodes_dict['paper']\n",
        "original_num_edges = data.edge_index_dict[('paper', 'cites', 'paper')].shape[1]\n",
        "original_num_labels = data.y_dict['paper'].shape[0]\n",
        "\n",
        "if num_nodes == original_num_nodes and num_edges == original_num_edges and num_labels == original_num_labels:\n",
        "    print(\"El dataset creado tiene la misma cantidad de datos que el dataset original.\")\n",
        "else:\n",
        "    print(\"El dataset creado no tiene la misma cantidad de datos que el dataset original.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNiQ_mbG0IJY",
        "outputId": "e63a522f-11eb-461e-9e0f-e5a7b9f9ae93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del dataset creado: 3\n",
            "Cantidad de nodos: 736389\n",
            "Cantidad de aristas: 5416271\n",
            "Cantidad de etiquetas: 736389\n",
            "El dataset creado tiene la misma cantidad de datos que el dataset original.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿De qué tamaño queda el dataset creado?\n",
        "\n",
        "R. El dataset creado tiene un tamaño de 3. Este número probablemente se refiere a la cantidad de elementos en el objeto Data (es decir, x, y y edge_index).\n",
        "\n",
        "Nos quedan 736389 nodos y 5416271 relaciones.\n",
        "\n",
        "2. ¿Cuántos nodos, aristas y labels tiene?\n",
        "\n",
        "Nodos: 736389. Aristas: 5416271. Labels: 736389. Caracteristicas por nodo: 128.\n",
        "\n",
        "3. Comparando el nuevo dataset con el original, pero considerando solo los tipos en común, ¿Tenemos la misma cantidad de datos que el dataset original? ¿Por qué?\n",
        "\n",
        "R. Tenemos menor cantidad de datos. Esto debido a que solo estamos tomando en cuenta nodos llamados paper citados por otros papers. Por lo tanto es de esperar que contenemos con una substancial menor cantidad de datos."
      ],
      "metadata": {
        "id": "yG32wihHHE_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data augmentation"
      ],
      "metadata": {
        "id": "Vuzm_FxkGzYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para aprovechar de mejor manera la información del grafo original, queremos obtener información adicional a partir de los datos que no estamos utilizando.\n",
        "\n",
        "En primer lugar, obtén del dataset todas las aristas que relacionan los papers con sus autores. Puede que te sea de utilidad, para el paso siguiente, generar un diccionario o estructura similar que relacione a cada autor del grafo con todos los papers que escribió.\n",
        "\n",
        "En segundo lugar, crea un nuevo dataset, que contenga toda la información del que acabas de crear en el paso 2. y agrega a el aristas entre todos los papers que tengan el mismo autor."
      ],
      "metadata": {
        "id": "GzJ66VFiHk_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Obtener aristas de papers con autores\n",
        "author_paper_edges = data.edge_index_dict[('author', 'writes', 'paper')]"
      ],
      "metadata": {
        "id": "e1bPhvQsG12B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear diccionario de autores y sus papers\n",
        "author_papers = {}\n",
        "for i in range(author_paper_edges.size(1)):\n",
        "    author_idx = author_paper_edges[0, i].item()\n",
        "    paper_idx = author_paper_edges[1, i].item()\n",
        "    if author_idx not in author_papers:\n",
        "        author_papers[author_idx] = [paper_idx]\n",
        "    else:\n",
        "        author_papers[author_idx].append(paper_idx)"
      ],
      "metadata": {
        "id": "ura7ovLbVqbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los primeros 5 autores y los papers asociados\n",
        "for author_idx in list(author_papers.keys())[:5]:\n",
        "    papers = author_papers[author_idx]\n",
        "    print(f\"Autor {author_idx}: Papers {papers}\")"
      ],
      "metadata": {
        "id": "GLFN96yjHeVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fce48f2-3b13-406b-cee8-9e814a49b176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autor 0: Papers [19703, 289285, 311768, 402711]\n",
            "Autor 1: Papers [181505, 297095, 336569]\n",
            "Autor 2: Papers [14217, 14630, 15376, 15420, 20656, 27715, 37354, 62987, 78157, 86646, 91551, 103412, 129861, 148900, 178805, 190892, 194646, 261498, 301784, 357433, 432992, 455787, 489838, 519995, 540904, 599199, 600495, 614248, 617244, 647287, 689138, 705788, 722195, 729975]\n",
            "Autor 3: Papers [230166, 392216, 504678, 625373]\n",
            "Autor 4: Papers [240904, 393817, 499283, 538604, 621886, 709654]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenar las nuevas aristas con las aristas originales\n",
        "edge_index = torch.cat([edge_index, torch.tensor(same_author_edges).t()], dim=1)\n",
        "\n",
        "# Crear el nuevo dataset con la información actualizada\n",
        "new_data = Data(x=x, y=y, edge_index=edge_index)"
      ],
      "metadata": {
        "id": "ZjAbcM1mZrkE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "52ba34e4-23b1-476b-8ab2-5657ba694333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e8966e0385a0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Concatenar las nuevas aristas con las aristas originales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msame_author_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Crear el nuevo dataset con la información actualizada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'same_author_edges' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.edge_index_dict.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkPQTIEWgN-6",
        "outputId": "e3cbca68-4569-4ae5-c73c-fa6d3c2caa28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([('author', 'affiliated_with', 'institution'), ('author', 'writes', 'paper'), ('paper', 'cites', 'paper'), ('paper', 'has_topic', 'field_of_study')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las aristas de autor a paper\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "author_paper_edge_index = data.edge_index_dict[('author', 'writes', 'paper')]\n"
      ],
      "metadata": {
        "id": "YczgDOmBZzxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un diccionario que mapea cada autor a todos los papers que escribió\n",
        "author_to_papers = {}\n",
        "for i in range(author_paper_edge_index.shape[1]):\n",
        "    paper_node = author_paper_edge_index[0, i].item()\n",
        "    author_node = author_paper_edge_index[1, i].item()\n",
        "    if author_node not in author_to_papers:\n",
        "        author_to_papers[author_node] = []\n",
        "    author_to_papers[author_node].append(paper_node)\n"
      ],
      "metadata": {
        "id": "YXO2Bm-Sgy8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterar sobre los elementos del diccionario\n",
        "for i, (author, papers) in enumerate(author_to_papers.items()):\n",
        "    # Imprimir el autor y sus papers\n",
        "    print(f\"Autor {author}: {papers}\")\n",
        "\n",
        "    # Interrumpir el bucle después de imprimir 5 autores\n",
        "    if i == 4:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOmGoMUwhcyW",
        "outputId": "77f6a9fe-b8f5-447e-b7c8-80f7cf6c643d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autor 19703: [0, 41685, 84368, 360028, 387940]\n",
            "Autor 289285: [0, 150451, 360028, 387940]\n",
            "Autor 311768: [0, 150451, 360028, 387940, 1071545]\n",
            "Autor 402711: [0, 360028, 387940]\n",
            "Autor 181505: [1, 912752, 1045321]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una lista para almacenar las nuevas aristas entre papers\n",
        "new_edges = []\n",
        "\n",
        "# Para cada autor, seleccionar cada par de papers adyacentes y agregar una arista entre ellos\n",
        "for author, papers in author_to_papers.items():\n",
        "    if len(papers) > 1:\n",
        "        # Ordenar los papers por su índice\n",
        "        sorted_papers = sorted(papers)\n",
        "        # Agregar una arista entre cada par de papers adyacentes\n",
        "        for i in range(len(sorted_papers) - 1):\n",
        "            new_edges.append((sorted_papers[i], sorted_papers[i+1]))\n",
        "\n",
        "# Convertir la lista de tuplas en un tensor\n",
        "new_edges_tensor = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Agregar las nuevas aristas al dataset existente\n",
        "augmented_edge_index = torch.cat([new_data.edge_index, new_edges_tensor], dim=1)\n",
        "\n",
        "# Crear un nuevo objeto Data con la información adicional\n",
        "augmented_data = Data(x=new_data.x, y=new_data.y, edge_index=augmented_edge_index)\n"
      ],
      "metadata": {
        "id": "CZZIKPDykpbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamaño del dataset creado\n",
        "size_dataset = len(augmented_data)\n",
        "print(\"Tamaño del dataset creado:\", size_dataset)\n",
        "\n",
        "# Cantidad de nodos, aristas y etiquetas en el dataset creado\n",
        "num_nodes = augmented_data.num_nodes\n",
        "num_edges = augmented_data.num_edges\n",
        "num_labels = augmented_data.y.shape[0]\n",
        "print(\"Cantidad de nodos:\", num_nodes)\n",
        "print(\"Cantidad de aristas:\", num_edges)\n",
        "print(\"Cantidad de etiquetas:\", num_labels)\n",
        "\n",
        "# Comparación con el dataset del paso 2\n",
        "if num_nodes == new_data.num_nodes and num_labels == new_data.y.shape[0]:\n",
        "    print(\"El dataset creado tiene la misma cantidad de nodos y etiquetas que el dataset del paso 2.\")\n",
        "else:\n",
        "    print(\"El dataset creado no tiene la misma cantidad de nodos y etiquetas que el dataset del paso 2.\")\n",
        "if num_edges > new_data.num_edges:\n",
        "    print(\"El dataset creado tiene más aristas que el dataset del paso 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJaZbm01z0SX",
        "outputId": "89383bd2-ed6f-4252-9ca5-7cb2352ef9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del dataset creado: 3\n",
            "Cantidad de nodos: 736389\n",
            "Cantidad de aristas: 11825542\n",
            "Cantidad de etiquetas: 736389\n",
            "El dataset creado tiene la misma cantidad de nodos y etiquetas que el dataset del paso 2.\n",
            "El dataset creado tiene más aristas que el dataset del paso 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿De qué tamaño queda el dataset creado?\n",
        "\n",
        "R. El dataset creado tiene un tamaño de 3. Este número se refiere a la cantidad de elementos en el objeto Data (es decir, x, y y edge_index).\n",
        "\n",
        "2. ¿Cuántos nodos, aristas y labels tiene?\n",
        "\n",
        "R. Nodos: 736389\n",
        "Aristas: 11825542\n",
        "Etiquetas (labels): 736389\n",
        "\n",
        "3. Comparando el nuevo dataset con el del paso 2., ¿Tenemos la misma cantidad de datos que el dataset original? ¿Por qué?\n",
        "\n",
        "R. Comparando el nuevo dataset con el del paso 2:\n",
        "\n",
        "La cantidad de nodos y etiquetas en el nuevo dataset es la misma que en el dataset del paso 2. Esto se debe a que no se añadió ningún nodo nuevo ni se cambiaron las etiquetas.\n",
        "La cantidad de aristas en el nuevo dataset es mayor que en el dataset del paso 2. Esto se debe a que se agregaron nuevas aristas entre los papers escritos por el mismo autor, lo que aumentó la cantidad total de aristas en el grafo.\n",
        "Por lo tanto, a pesar de que el nuevo dataset tiene la misma cantidad de nodos y etiquetas que el dataset del paso 2, contiene más información al tener más conexiones (aristas) entre los nodos. Esta información adicional puede ser muy útil para mejorar la eficacia de los modelos de aprendizaje automático basados en grafos."
      ],
      "metadata": {
        "id": "YmRYg-ISIdbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Split del dataset\n"
      ],
      "metadata": {
        "id": "rJOfSlXRG8X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga el split precomputado desde el archivo `split`, nuevamente puedes descargar el archivo desde [este link](https://drive.google.com/file/d/1plLjfKhU7XcJshLvwZaoavdC_u8tL48z) o usar el comando que se encuentra a continuación."
      ],
      "metadata": {
        "id": "0SDzF0RANetY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1plLjfKhU7XcJshLvwZaoavdC_u8tL48z"
      ],
      "metadata": {
        "id": "_vJ8SX31Oeed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549146ff-4517-48e7-dd60-27b447e7ec48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1plLjfKhU7XcJshLvwZaoavdC_u8tL48z\n",
            "To: /content/split\n",
            "100% 5.89M/5.89M [00:00<00:00, 17.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive//Backup/split', 'rb') as file:\n",
        "  split_data = pickle.load(file)\n",
        "\n",
        "print(split_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6B5FNrK0QX8",
        "outputId": "fd576e01-3f89-40e7-e4b1-96e5e96cc508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': {'paper': tensor([     0,      1,      2,  ..., 736386, 736387, 736388])}, 'valid': {'paper': tensor([   332,    756,    784,  ..., 736364, 736367, 736370])}, 'test': {'paper': tensor([   359,    411,    608,  ..., 736358, 736384, 736385])}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(split_data))\n",
        "print(split_data.keys())\n",
        "print(type(split_data['train']))\n",
        "print(len(split_data['train']['paper']))\n",
        "print(len(split_data['test']['paper']))\n",
        "print(len(split_data['valid']['paper']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQd_f8aF0Z4C",
        "outputId": "578cf4a6-a050-464a-af06-138d26be0615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['train', 'valid', 'test'])\n",
            "<class 'dict'>\n",
            "629571\n",
            "41939\n",
            "64879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿En qué formato se encuentra guardado el split?\n",
        "\n",
        "R.El split se encuentra guardado en formato de diccionario de Python, donde cada clave del diccionario representa una de las partes del split ('train', 'valid', y 'test') y su valor asociado es otro diccionario que contiene el tipo de nodo y un tensor con los índices de los nodos correspondientes a esa parte.\n",
        "\n",
        "2. ¿En cuántas partes se dividirá el dataset?\n",
        "\n",
        "R. El dataset se dividirá en tres partes: entrenamiento ('train'), validación ('valid') y prueba ('test').\n",
        "\n",
        "3. ¿Qué porcentaje del dataset se asignó a cada parte?\n",
        "\n",
        "R. Nodos asignados al entrenamiento: 629571 (85.49%)\n",
        "Nodos asignados a la validación: 64879 (8.81%)\n",
        "Nodos asignados a la prueba: 41939 (5.70%)"
      ],
      "metadata": {
        "id": "kPOYINNwO311"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar el tipo de objeto que se cargó\n",
        "print(\"Tipo de objeto cargado:\", type(split_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42kDDOLelj37",
        "outputId": "5e5ada48-8bda-46e1-b7a4-2eb1b0e3da4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipo de objeto cargado: <class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si el objeto es un diccionario, imprimir las claves\n",
        "if isinstance(split_data, dict):\n",
        "    print(\"Claves del diccionario:\", split_data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icuHrh79lyXB",
        "outputId": "ab74103d-4d00-47ec-f898-5f61a87b08b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claves del diccionario: dict_keys(['train', 'valid', 'test'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive//Backup/split', 'rb') as file:\n",
        "  split = pickle.load(file)\n",
        "\n",
        "print(split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_rViK4JmOuX",
        "outputId": "93a6f33a-972e-4b77-ded6-0d8c782f7283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': {'paper': tensor([     0,      1,      2,  ..., 736386, 736387, 736388])}, 'valid': {'paper': tensor([   332,    756,    784,  ..., 736364, 736367, 736370])}, 'test': {'paper': tensor([   359,    411,    608,  ..., 736358, 736384, 736385])}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de nodos asignados a cada parte del split\n",
        "train_nodes = len(split['train'])\n",
        "valid_nodes = len(split['valid'])\n",
        "test_nodes = len(split['test'])\n",
        "\n",
        "# Porcentaje del dataset asignado a cada parte\n",
        "total_nodes = train_nodes + valid_nodes + test_nodes\n",
        "train_percentage = (train_nodes / total_nodes) * 100\n",
        "valid_percentage = (valid_nodes / total_nodes) * 100\n",
        "test_percentage = (test_nodes / total_nodes) * 100\n",
        "\n",
        "print(f\"Nodos asignados al entrenamiento: {train_nodes} ({train_percentage:.2f}%)\")\n",
        "print(f\"Nodos asignados a la validación: {valid_nodes} ({valid_percentage:.2f}%)\")\n",
        "print(f\"Nodos asignados a la prueba: {test_nodes} ({test_percentage:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z3Tf0Prl7sC",
        "outputId": "f624d604-e21a-42c2-92e0-5a4146d0a3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodos asignados al entrenamiento: 1 (33.33%)\n",
            "Nodos asignados a la validación: 1 (33.33%)\n",
            "Nodos asignados a la prueba: 1 (33.33%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nodos de entrenamiento:\", split['train'])\n",
        "print(\"Nodos de validación:\", split['valid'])\n",
        "print(\"Nodos de prueba:\", split['test'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRrois5Nmc6b",
        "outputId": "8a60cb8d-4467-4906-b495-18ea6df959a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodos de entrenamiento: {'paper': tensor([     0,      1,      2,  ..., 736386, 736387, 736388])}\n",
            "Nodos de validación: {'paper': tensor([   332,    756,    784,  ..., 736364, 736367, 736370])}\n",
            "Nodos de prueba: {'paper': tensor([   359,    411,    608,  ..., 736358, 736384, 736385])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tipo de los elementos en el conjunto de entrenamiento:\", type(split['train'][0]))\n",
        "print(\"Tipo de los elementos en el conjunto de validación:\", type(split['valid'][0]))\n",
        "print(\"Tipo de los elementos en el conjunto de prueba:\", type(split['test'][0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "4cjM8u-zmhSs",
        "outputId": "c8e20f0d-91f2-42a0-d50d-5eb0cd2f7f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1980bf113b1c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de los elementos en el conjunto de entrenamiento:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de los elementos en el conjunto de validación:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de los elementos en el conjunto de prueba:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de nodos asignados a cada parte del split\n",
        "train_nodes = len(split['train']['paper'])\n",
        "valid_nodes = len(split['valid']['paper'])\n",
        "test_nodes = len(split['test']['paper'])\n",
        "\n",
        "print(f\"Nodos asignados al entrenamiento: {train_nodes} ({100 * train_nodes / num_nodes:.2f}%)\")\n",
        "print(f\"Nodos asignados a la validación: {valid_nodes} ({100 * valid_nodes / num_nodes:.2f}%)\")\n",
        "print(f\"Nodos asignados a la prueba: {test_nodes} ({100 * test_nodes / num_nodes:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2NIhhGSm1gZ",
        "outputId": "b4695fa8-1e24-4870-eba8-f22d5f97d25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodos asignados al entrenamiento: 629571 (85.49%)\n",
            "Nodos asignados a la validación: 64879 (8.81%)\n",
            "Nodos asignados a la prueba: 41939 (5.70%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "WTH145bZJ7rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Creacion de la red"
      ],
      "metadata": {
        "id": "RhZ55TKfNca7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN"
      ],
      "metadata": {
        "id": "hjwQPfbfnK9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea una GCN, con `GCNConv`, que reciba como parámetro la cantidad de layer convolucionales internas que utilizará.\n",
        "\n",
        "La GCN debe utilizar una función de activación ELu y un dropout entre cada layer; la probabilidad del dropout debe ser un parámetro de la red.\n",
        "\n",
        "Cada capa debe normalizar los datos y las capas que debes crear son:\n",
        "\n",
        "1. Una capa convolucional que tenga `in_channels` de entrada y `hidden_channels` de salida.\n",
        "2. Un cantidad `hidden_layers` de capas convolucionales intermedias con `hidden_channels` de entrada y de salida.\n",
        "3. Una capa convolucional que tenga `hidden_channels` de entrada y `out_channels` de salida.\n",
        "\n",
        "Asegúrate que las capas internas sean parte del modelo."
      ],
      "metadata": {
        "id": "LEDWuxj6fMAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import ELU, Dropout\n",
        "from torch_geometric.nn import GCNConv, BatchNorm\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_hidden_layers, dropout_prob):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # Crear las capas convolucionales\n",
        "        self.conv_in = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv_out = GCNConv(hidden_channels, out_channels)\n",
        "        self.conv_hidden = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_hidden_layers)])\n",
        "\n",
        "        # Crear las capas de batch normalization\n",
        "        self.batch_norm_in = BatchNorm(hidden_channels)\n",
        "        self.batch_norm_hidden = torch.nn.ModuleList([BatchNorm(hidden_channels) for _ in range(num_hidden_layers)])\n",
        "\n",
        "        # Crear las capas de activación y dropout\n",
        "        self.activation = ELU()\n",
        "        self.dropout = Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Capa de entrada\n",
        "        x = self.conv_in(x, edge_index)\n",
        "        x = self.batch_norm_in(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Capas ocultas\n",
        "        for conv, batch_norm in zip(self.conv_hidden, self.batch_norm_hidden):\n",
        "            x = conv(x, edge_index)\n",
        "            x = batch_norm(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Capa de salida\n",
        "        x = self.conv_out(x, edge_index)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6xXwR1PHdlvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea una instancia de tu modelo con 256 `hidden_channels`, 349 `out_channels`, 3 `hidden_layers` y una probabilidad de dropout de 0.3.\n",
        "\n",
        "Imprime tu instancia del modelo."
      ],
      "metadata": {
        "id": "8B3DtDccJi9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros\n",
        "in_channels = data.num_node_features  # Dependiendo de la representación de tus nodos, necesitarás ajustar esto.\n",
        "hidden_channels = 256\n",
        "out_channels = 349\n",
        "num_hidden_layers = 3\n",
        "dropout_prob = 0.3\n",
        "\n",
        "# Crear la instancia del modelo\n",
        "model = GCN(in_channels, hidden_channels, out_channels, num_hidden_layers, dropout_prob)\n",
        "\n",
        "# Imprimir la instancia del modelo\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "Wcq-gV-WkCnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f9bee9-5872-4a57-c83a-1f1773307abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv_in): GCNConv(0, 256)\n",
            "  (conv_out): GCNConv(256, 349)\n",
            "  (conv_hidden): ModuleList(\n",
            "    (0-2): 3 x GCNConv(256, 256)\n",
            "  )\n",
            "  (batch_norm_in): BatchNorm(256)\n",
            "  (batch_norm_hidden): ModuleList(\n",
            "    (0-2): 3 x BatchNorm(256)\n",
            "  )\n",
            "  (activation): ELU(alpha=1.0)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿Cuántas capas tiene, en total, el modelo?\n",
        "\n",
        "R. En el modelo, hay un total de 5 capas convolucionales: una capa de entrada (conv_in), tres capas ocultas (conv_hidden) y una capa de salida (conv_out). Las capas de normalización por lotes y las funciones de activación y dropout, aunque son partes cruciales de la arquitectura de la red, a menudo no se cuentan como \"capas\" en el mismo sentido. Pero si consideramos \"en total\", el modelo tiene 9 \"capas\" incluyendo las capas de normalización por lotes. Y si además decidimos contar las funciones de activación y dropout como capas, entonces tendríamos un total de 11 capas\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KxULIINnKbW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sage"
      ],
      "metadata": {
        "id": "ojWqxLaVo732"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea una GCN, con `SAGEConv`, que reciba como parámetro la cantidad de layers convolucionales internas que utilizará.\n",
        "\n",
        "La GCN debe utilizar una función de activación ELu y un dropout entre cada layer; la probabilidad del dropout debe ser un parámetro de la red.\n",
        "\n",
        "Debes incluír las siguientes capas:\n",
        "\n",
        "1. Una capa convolucional que tenga `in_channels` de entrada y `hidden_channels` de salida.\n",
        "2. Un cantidad `hidden_layers` de capas convolucionales intermedias con `hidden_channels` de entrada y de salida.\n",
        "3. Una capa convolucional que tenga `hidden_channels` de entrada y `out_channels` de salida.\n",
        "\n",
        "Instancia tu modelo con 256 `hidden_channels`, 349 `out_channels`, 4 `hidden_layers` y una probabilidad de dropout de 0.5"
      ],
      "metadata": {
        "id": "20aBV-FTo-DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, hidden_layers, dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        # Primera capa convolucional\n",
        "        self.conv_in = SAGEConv(in_channels, hidden_channels)\n",
        "\n",
        "        # Capas convolucionales ocultas\n",
        "        self.conv_hidden = torch.nn.ModuleList()\n",
        "        for _ in range(hidden_layers):\n",
        "            self.conv_hidden.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        # Capa convolucional de salida\n",
        "        self.conv_out = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "        # Función de activación ELU\n",
        "        self.activation = torch.nn.ELU()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Primera capa convolucional con activación y dropout\n",
        "        x = self.conv_in(x, edge_index)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Capas convolucionales ocultas con activación y dropout\n",
        "        for conv in self.conv_hidden:\n",
        "            x = conv(x, edge_index)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Capa convolucional de salida\n",
        "        x = self.conv_out(x, edge_index)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZN9E2kyUo_Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SAGE(in_channels=256, hidden_channels=256, out_channels=349, hidden_layers=4, dropout=0.5)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "pwCmQM4NLhTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe99812-0789-43e6-c4cd-08ca20fc24db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAGE(\n",
            "  (conv_in): SAGEConv(256, 256, aggr=mean)\n",
            "  (conv_hidden): ModuleList(\n",
            "    (0-3): 4 x SAGEConv(256, 256, aggr=mean)\n",
            "  )\n",
            "  (conv_out): SAGEConv(256, 349, aggr=mean)\n",
            "  (activation): ELU(alpha=1.0)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿Cuántas capas tiene, en total, el modelo?\n",
        "\n",
        "R. El modelo GraphSAGE tiene un total de 6 capas. Aquí está el desglose:\n",
        "\n",
        "Una capa convolucional de entrada (conv_in): Esta capa convierte las características de entrada de 256 dimensiones a las características ocultas de 256 dimensiones.\n",
        "\n",
        "Cuatro capas convolucionales ocultas (conv_hidden): Cada una de estas capas toma características de 256 dimensiones y las transforma en características de 256 dimensiones.\n",
        "\n",
        "Una capa convolucional de salida (conv_out): Esta capa convierte las características ocultas de 256 dimensiones a las características de salida de 349 dimensiones.\n",
        "\n",
        "Además, cada una de estas capas convolucionales es seguida por una función de activación ELU y un dropout. Sin embargo, estas no son consideradas capas en el sentido tradicional."
      ],
      "metadata": {
        "id": "YCZXoPNWKz_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Entrenamiento de la red"
      ],
      "metadata": {
        "id": "F6gz-bBlpMij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea dos instancias de cada red y entrénalas utilizando los dos datsets creados anteriormente.\n",
        "\n",
        "Puedes utilizar la función de activación, optimizador, capas ocultas y learning rate que prefieras, pero debes entrenar por al menos 100 épocas y asegurarte de que la red efectivamente esté aprendiendo (la función de pérdida vaya bajando al menos al inicio del entrenamiento).\n",
        "\n",
        "**Importante:** El dataset cuenta con 349 clases distintas para la clasificación"
      ],
      "metadata": {
        "id": "eukNKDPgpPC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n"
      ],
      "metadata": {
        "id": "-e6HMG-sP6GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(in_channels=data.num_features, hidden_channels=256,\n",
        "                out_channels=349, num_layers=2,\n",
        "                dropout=0.5)"
      ],
      "metadata": {
        "id": "lV5t3hERpPML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv_in = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv_hidden = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.conv_hidden.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.conv_out = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.activation = torch.nn.ELU()\n",
        "        self.batch_norm_in = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.batch_norm_hidden = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.batch_norm_hidden.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv_in(x, edge_index)\n",
        "        x = self.batch_norm_in(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        for conv, batch_norm in zip(self.conv_hidden, self.batch_norm_hidden):\n",
        "            x = conv(x, edge_index)\n",
        "            x = batch_norm(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "KEpfXMxRq__J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "        self.conv_in = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv_hidden = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.conv_hidden.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.conv_out = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.activation = torch.nn.ELU()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv_in(x, edge_index)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        for conv in self.conv_hidden:\n",
        "            x = conv(x, edge_index)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "gkA1Y5HhrTdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Definir una función de entrenamiento\n",
        "def train(model, loader, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Entrenar los modelos\n",
        "for i in range(len(models)):\n",
        "    # Asegúrate de usar el mismo optimizador y condiciones de entrenamiento\n",
        "    optimizer = torch.optim.Adam(models[i].parameters(), lr=0.01)\n",
        "\n",
        "    # Crear DataLoader para el dataset\n",
        "    loader = DataLoader(datasets[i], batch_size=32, shuffle=True)\n",
        "\n",
        "    for epoch in range(100):  # Número de épocas\n",
        "        loss = train(models[i], loader, optimizer)\n",
        "        # puedes agregar código aquí para registrar el rendimiento y hacer un seguimiento del aprendizaje\n",
        "        print(f\"Model {i+1}, Epoch {epoch+1}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "# Crear las instancias de los modelos\n",
        "gcn_model1 = GCN(in_channels=data.num_features, hidden_channels=256,\n",
        "                out_channels=349, num_layers=2,\n",
        "                dropout=0.5)\n",
        "\n",
        "gcn_model2 = GCN(in_channels=data.num_features, hidden_channels=256,\n",
        "                out_channels=349, num_layers=2,\n",
        "                dropout=0.5)\n",
        "\n",
        "sage_model1 = SAGE(in_channels=data.num_features, hidden_channels=256,\n",
        "                   out_channels=349, num_layers=2,\n",
        "                   dropout=0.5)\n",
        "\n",
        "sage_model2 = SAGE(in_channels=data.num_features, hidden_channels=256,\n",
        "                   out_channels=349, num_layers=2,\n",
        "                   dropout=0.5)\n",
        "\n",
        "# Lista de modelos y datasets para iterar a través de ellos\n",
        "models = [gcn_model1, gcn_model2, sage_model1, sage_model2]\n",
        "datasets = [original_dataset, augmented_dataset, original_dataset, augmented_dataset]\n",
        "\n",
        "# Entrenar los modelos\n",
        "for i in range(len(models)):\n",
        "    # Asegurarte de usar el mismo optimizador y condiciones de entrenamiento\n",
        "    optimizer = torch.optim.Adam(models[i].parameters(), lr=0.01)\n",
        "    for epoch in range(100):  # Número de épocas\n",
        "        train(models[i], datasets[i], split_dict['train'], optimizer)\n",
        "        # puedes agregar código aquí para registrar el rendimiento y hacer un seguimiento del aprendizaje\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "5A-3TT3VrCRV",
        "outputId": "e124d673-2481-4c98-8610-0a9388f2e1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-65474b1a0051>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Lista de modelos y datasets para iterar a través de ellos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgcn_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcn_model2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msage_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msage_model2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moriginal_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Entrenar los modelos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'original_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preguntas\n",
        "\n",
        "1. ¿Qué combinación de red-dataset obtiene mejor accuracy?\n",
        "2. Comparando el dataset aumentado con el original, ¿Presenta alguna ventaja?, puedes considerar la velocidad de entrenamiento, el reultado final de la red o cualquier otro parámetro que creas significativo.\n",
        "3. ¿El aumento en el dataset afecta de la misma manera a ambas redes? ¿Es significativa la diferencia?\n",
        "4. Comparando los dos tipos de red, ¿Es significativa la diferencia entre los accuracy?"
      ],
      "metadata": {
        "id": "ahRuTBRrMrpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Uso de batches en grafos"
      ],
      "metadata": {
        "id": "P8q-hXV9pqwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Carga del dataset"
      ],
      "metadata": {
        "id": "-SMNT1IS3xWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando `torch.geometric`, carga el dataset `Mutagenicity` de `TUDataset`"
      ],
      "metadata": {
        "id": "ltqJv2-M3xWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "# Cargar el dataset Mutagenicity\n",
        "dataset = TUDataset(root='/tmp/Mutagenicity', name='Mutagenicity')\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n"
      ],
      "metadata": {
        "id": "7fh_-rK93xWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a2cd48-9be4-4a39-d566-700d08e42775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/Mutagenicity.zip\n",
            "Extracting /tmp/Mutagenicity/Mutagenicity/Mutagenicity.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Mutagenicity(4337):\n",
            "======================\n",
            "Number of graphs: 4337\n",
            "Number of features: 14\n",
            "Number of classes: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "Una vez cargado el dataset debes responder las siguientes preguntas:\n",
        "\n",
        "1. ¿De qué tipo de objeto es el dataset cargado?\n",
        "\n",
        "R. El dataset cargado es de tipo torch_geometric.datasets.tu_dataset.TUDataset.\n",
        "\n",
        "2. ¿Cuántos grafos tiene el dataset?\n",
        "\n",
        "R. El dataset contiene 4337 grafos.\n",
        "\n",
        "3. ¿Cuántos nodos y aristas tiene, en promedio, cada grafo?\n",
        "\n",
        "R. En promedio, cada grafo en el conjunto de datos tiene aproximadamente 30.32 nodos y 61.54 aristas.\n",
        "\n",
        "4. Elige un grafo cualquiera del dataset (puedes ser el primero) y responde:\n",
        "    1. ¿Cuántos nodos y aristas tiene el grafo?\n",
        "\n",
        "    R. El grafo tiene 16 nodos y 32 aristas.\n",
        "\n",
        "    2. ¿El grafo tiene self loops?\n",
        "\n",
        "    R. El grafo no tiene self-loops. Un self-loop es una arista que se conecta a un nodo consigo mismo.\n",
        "\n",
        "    3. ¿El grafo es dirigido?\n",
        "\n",
        "    R. El grafo no es dirigido. En un grafo dirigido, las aristas tienen una dirección asociada (van de un nodo a otro). En cambio, en este caso, las aristas no tienen una dirección, lo que significa que la conexión entre dos nodos es bidireccional.\n",
        "\n",
        "    4. ¿El grafo tiene nodos aislados?\n",
        "    \n",
        "    R. El grafo no tiene nodos aislados. Un nodo aislado es un nodo que no tiene aristas conectadas a él\n"
      ],
      "metadata": {
        "id": "YG_MeHh6PBbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipo de objeto del dataset\n",
        "print(f\"Tipo de objeto del dataset: {type(dataset)}\")\n",
        "\n",
        "# Cantidad de grafos en el dataset\n",
        "print(f\"Cantidad de grafos en el dataset: {len(dataset)}\")\n",
        "\n",
        "# Promedio de nodos y aristas en los grafos del dataset\n",
        "num_nodes = [data.num_nodes for data in dataset]\n",
        "num_edges = [data.num_edges for data in dataset]\n",
        "avg_nodes = sum(num_nodes) / len(dataset)\n",
        "avg_edges = sum(num_edges) / len(dataset)\n",
        "\n",
        "print(f\"Número promedio de nodos por grafo: {avg_nodes}\")\n",
        "print(f\"Número promedio de aristas por grafo: {avg_edges}\")\n",
        "\n",
        "# Selecciona un grafo del dataset (p. ej., el primero)\n",
        "data = dataset[0]\n",
        "\n",
        "# Características del grafo seleccionado\n",
        "print(f\"\\nPara el primer grafo del dataset:\")\n",
        "print(f\"Número de nodos: {data.num_nodes}\")\n",
        "print(f\"Número de aristas: {data.num_edges}\")\n",
        "print(f\"El grafo tiene self loops: {'Sí' if data.contains_self_loops() else 'No'}\")\n",
        "print(f\"El grafo es dirigido: {'Sí' if data.is_directed() else 'No'}\")\n",
        "print(f\"El grafo tiene nodos aislados: {'Sí' if data.contains_isolated_nodes() else 'No'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sWLytL9s_k_",
        "outputId": "e6a53b24-3719-442b-d6f7-f0dc32a467d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipo de objeto del dataset: <class 'torch_geometric.datasets.tu_dataset.TUDataset'>\n",
            "Cantidad de grafos en el dataset: 4337\n",
            "Número promedio de nodos por grafo: 30.317731150564907\n",
            "Número promedio de aristas por grafo: 61.53885174083468\n",
            "\n",
            "Para el primer grafo del dataset:\n",
            "Número de nodos: 16\n",
            "Número de aristas: 32\n",
            "El grafo tiene self loops: No\n",
            "El grafo es dirigido: No\n",
            "El grafo tiene nodos aislados: No\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Split del dataset"
      ],
      "metadata": {
        "id": "d4jQfCac3xWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide aleatoriamente el dataset entre 3 sets, entrenamiento, validación y test. Asigna un 80%, 10% y 10% de los datos a cada set, respectivamente.\n",
        "\n",
        "Intenta que la distribución quede relativamente uniforme respecto al tamaño promedio de cada set."
      ],
      "metadata": {
        "id": "83TD_1OT3xWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "torch.manual_seed(0)  # Para que el split sea reproducible\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "f7sV_cyV3xWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba454c8-a43d-4301-bdc4-988d66d0668c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 3469\n",
            "Validation dataset size: 434\n",
            "Testing dataset size: 434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿Cuántos grafos tiene cada dataset?\n",
        "\n",
        "R. Cada dataset tiene la siguiente cantidad de grafos:\n",
        "\n",
        "El dataset de entrenamiento tiene 3469 grafos.\n",
        "El dataset de validación tiene 434 grafos.\n",
        "El dataset de prueba tiene 434 grafos.\n",
        "\n",
        "2. ¿Cuántos nodos y aristas tiene, en promedio, cada grafo?\n",
        "\n",
        "R. Cada grafo en los datasets tiene, en promedio, la siguiente cantidad de nodos y aristas:\n",
        "\n",
        "En el dataset de entrenamiento, cada grafo tiene en promedio 30.02 nodos y 61.19 aristas.\n",
        "En el dataset de validación, cada grafo tiene en promedio 29.63 nodos y 60.63 aristas.\n",
        "En el dataset de prueba, cada grafo tiene en promedio 33.39 nodos y 65.25 aristas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7MXYV8QTPBjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de grafos en cada dataset\n",
        "print(f\"Number of graphs in the training dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of graphs in the validation dataset: {len(valid_dataset)}\")\n",
        "print(f\"Number of graphs in the test dataset: {len(test_dataset)}\")\n",
        "\n",
        "# Cálculo del número promedio de nodos y aristas en cada grafo\n",
        "def calculate_average_nodes_edges(graph_dataset):\n",
        "    total_nodes = 0\n",
        "    total_edges = 0\n",
        "    for data in graph_dataset:\n",
        "        total_nodes += data.num_nodes\n",
        "        total_edges += data.num_edges\n",
        "    avg_nodes = total_nodes / len(graph_dataset)\n",
        "    avg_edges = total_edges / len(graph_dataset)\n",
        "    return avg_nodes, avg_edges\n",
        "\n",
        "avg_nodes_train, avg_edges_train = calculate_average_nodes_edges(train_dataset)\n",
        "avg_nodes_valid, avg_edges_valid = calculate_average_nodes_edges(valid_dataset)\n",
        "avg_nodes_test, avg_edges_test = calculate_average_nodes_edges(test_dataset)\n",
        "\n",
        "print(f\"Average number of nodes in the training dataset: {avg_nodes_train}\")\n",
        "print(f\"Average number of edges in the training dataset: {avg_edges_train}\")\n",
        "print(f\"Average number of nodes in the validation dataset: {avg_nodes_valid}\")\n",
        "print(f\"Average number of edges in the validation dataset: {avg_edges_valid}\")\n",
        "print(f\"Average number of nodes in the test dataset: {avg_nodes_test}\")\n",
        "print(f\"Average number of edges in the test dataset: {avg_edges_test}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViwAvnPpti8x",
        "outputId": "04612e55-deff-4a51-8f63-8a80ffaefceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs in the training dataset: 3469\n",
            "Number of graphs in the validation dataset: 434\n",
            "Number of graphs in the test dataset: 434\n",
            "Average number of nodes in the training dataset: 30.01931392332084\n",
            "Average number of edges in the training dataset: 61.188238685500146\n",
            "Average number of nodes in the validation dataset: 29.629032258064516\n",
            "Average number of edges in the validation dataset: 60.62672811059908\n",
            "Average number of nodes in the test dataset: 33.39170506912443\n",
            "Average number of edges in the test dataset: 65.25345622119816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creación de los dataloaders\n"
      ],
      "metadata": {
        "id": "d-1Bztod3xWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea un `Dataloader` para cada dataset. Elige el número de batches de cada uno según creas conveniente."
      ],
      "metadata": {
        "id": "IF0EE-K83xWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Decide on the batch size. This may depend on the memory available on your machine.\n",
        "# A typical choice of batch size is between 32 and 128.\n",
        "batch_size = 32\n",
        "\n",
        "# Create a DataLoader for each dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Kf5df04m3xWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b40ecef-ff50-4713-dee3-9392d916419d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿Qué tamaños de batch utilizaste para cada set? Justifica el por qué elegiste cada uno de ellos y qué ventajas/desventajas tiene comparado con otros valores.\n",
        "\n",
        "R. El tamaño de los batches para cada set es 32. Este tamaño se elige comúnmente en las aplicaciones de aprendizaje automático por ser un número potencia de 2, lo cual resulta beneficioso para la optimización de la GPU. También ofrece un buen equilibrio entre la eficiencia del aprendizaje y los requisitos de memoria.\n",
        "\n",
        "Sin embargo, la elección del tamaño de batch puede depender de varios factores, como la memoria disponible en tu hardware, el tipo de modelo que se está entrenando, y el tamaño del conjunto de datos. Un tamaño de batch más grande puede llevar a un aprendizaje más rápido (menos iteraciones por época), pero también puede conducir a estimaciones de gradiente menos precisas y requerir más memoria. Por el contrario, un tamaño de batch más pequeño puede llevar a estimaciones de gradiente más precisas, pero puede requerir más iteraciones por época y ser menos eficiente en términos de computación.\n",
        "\n",
        "2. ¿Cuántos batches tiene cada uno de los sets?\n",
        "\n",
        "R. podemos ver que el conjunto de entrenamiento se divide en 109 batches, mientras que los conjuntos de validación y prueba se dividen en 14 batches cada uno.\n",
        "\n",
        "3. ¿De qué tamaño es cada batch resultante?\n",
        "\n",
        "R. Cada batch tiene un tamaño de 32, aunque el tamaño del último batch puede ser menor si el tamaño total del conjunto de datos no es un múltiplo de 32."
      ],
      "metadata": {
        "id": "JrJ8x3emPBnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# número de batches en el conjunto de entrenamiento\n",
        "num_batches_train = len(train_loader)\n",
        "\n",
        "# número de batches en el conjunto de validación\n",
        "num_batches_valid = len(valid_loader)\n",
        "\n",
        "# número de batches en el conjunto de prueba\n",
        "num_batches_test = len(test_loader)\n",
        "\n",
        "print(f\"Número de batches en el conjunto de entrenamiento: {num_batches_train}\")\n",
        "print(f\"Número de batches en el conjunto de validación: {num_batches_valid}\")\n",
        "print(f\"Número de batches en el conjunto de prueba: {num_batches_test}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-cXr1AIzCCo",
        "outputId": "e0d1e1af-a48d-4e2e-84b7-046d906d4042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de batches en el conjunto de entrenamiento: 109\n",
            "Número de batches en el conjunto de validación: 14\n",
            "Número de batches en el conjunto de prueba: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el tamaño del primer batch en el conjunto de entrenamiento\n",
        "first_batch = next(iter(train_loader))\n",
        "print(f\"El tamaño del primer batch en el conjunto de entrenamiento es: {len(first_batch)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Aq_HYGHzY1k",
        "outputId": "4edf5baf-9d2a-4e2c-eb9b-cc71efbc731f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tamaño del primer batch en el conjunto de entrenamiento es: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creacion de la red"
      ],
      "metadata": {
        "id": "3VRQ2VYI3xWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea una GCN, con `GCNConv`, que reciba como parámetro la cantidad de layers convolucionales que utilizará para crear los embeddings de cada grafo.\n",
        "\n",
        "La GCN debe utilizar una función de activación ReLu.\n",
        "\n",
        "Debe tener las siguientes capas:\n",
        "\n",
        "1. Una capa convolucional que tenga `in_channels` de entrada y `hidden_channels` de salida.\n",
        "2. Un cantidad `hidden_layers` de capas intermedias con `hidden_channels` de entrada y de salida.\n",
        "3. Un _mean pool_ que obtenga el mebedding del grafo a partir de los embeddings de los nodos.\n",
        "4. Un dropout antes de la última capa. La probabilidad del dropout debe ser un parámetro de la red.\n",
        "4. Una capa *fully connected* que tenga `hidden_channels` de entrada y `out_channels` de salida.\n",
        "\n",
        "Asegúrate que las capas internas sean parte del modelo."
      ],
      "metadata": {
        "id": "KB_XlR0A3xWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv_in = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv_hidden = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.fc_out = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Primera capa convolucional\n",
        "        x = self.conv_in(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Capas convolucionales intermedias\n",
        "        for conv in self.conv_hidden:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "\n",
        "        # Pooling de los embeddings de los nodos para obtener el embedding del grafo\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Dropout y última capa fully connected\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3Zf3VCw205bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea una instancia de tu modelo con `hidden_layers = 2` e imprime los detalles del modelo"
      ],
      "metadata": {
        "id": "TNycPh4tSTSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(in_channels=dataset.num_node_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=2, dropout=0.5)\n"
      ],
      "metadata": {
        "id": "4LEbP9iJSTaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T28biJ591KMn",
        "outputId": "c43d2206-da04-44bd-a08e-789ad3e87724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv_in): GCNConv(14, 64)\n",
            "  (conv_hidden): ModuleList(\n",
            "    (0): GCNConv(64, 64)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc_out): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "1. ¿Cuántas capas tiene el modelo?\n",
        "\n",
        "R. El modelo tiene 5 capas:\n",
        "\n",
        "La primera capa es una capa de convolución gráfica (GCNConv) que transforma las características de entrada a características ocultas.\n",
        "\n",
        "Luego tienes un número específico de capas ocultas (en este caso 2) que son capas de convolución gráfica que transforman las características ocultas en nuevas características ocultas. Por lo tanto, cada capa de estas cuenta como una capa adicional.\n",
        "\n",
        "Una capa de dropout que regulariza el modelo durante el entrenamiento, ayudando a prevenir el sobreajuste.\n",
        "\n",
        "Una capa de agrupación global (global mean pooling), que calcula el promedio de las características de los nodos para obtener una representación de todo el grafo.\n",
        "\n",
        "Finalmente, tienes una capa totalmente conectada (linear), que transforma las características del grafo (después de la agrupación) en la predicción de salida.\n",
        "\n",
        "Por lo tanto, en total, tu modelo tiene 5 capas."
      ],
      "metadata": {
        "id": "15GsEFxlPBq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Entrenamiento de la red"
      ],
      "metadata": {
        "id": "Z6S79m5J3xWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea tres instancias del a red, usando `hidden_layers = 0`, `hidden_layers = 2` y `hidden_layers = 4`.\n",
        "\n",
        "Entrena cada red por al menos 100 épocas, considerando el set de validación en el proceso. Los hiperparámetros, función de pérdida y optimizador quedan a tu criterio, pero debes asegurarte que la red aprende (la función de pérdida va bajando, al menos en las primeras etapas de entrenamiento)."
      ],
      "metadata": {
        "id": "Su5jM5w4DCpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Configuración de la pérdida y el optimizador\n",
        "criterion = CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Lista para almacenar los modelos\n",
        "models = []\n",
        "\n",
        "# Crear las instancias de los modelos y entrenarlos\n",
        "for hidden_layers in [0, 2, 4]:\n",
        "    model = GCN(in_channels=dataset.num_features, hidden_channels=256,\n",
        "                out_channels=dataset.num_classes, num_layers=hidden_layers,\n",
        "                dropout=0.5)\n",
        "    models.append(model)\n",
        "\n",
        "    # Configurar el optimizador\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    for epoch in range(100):\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "3x8l5qdI3xWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preguntas\n",
        "\n",
        "Una vez cargado el dataset debes responder las siguientes preguntas:\n",
        "\n",
        "1. En términos teóricos, ¿Qué representa la cantidad de `hidden_layers` del modelo?\n",
        "\n",
        "R. En términos teóricos, la cantidad de hidden_layers en un modelo de red neuronal representa la profundidad del modelo. En una Graph Convolutional Network (GCN), cada capa oculta puede considerarse como un paso de propagación de mensajes a través del grafo, donde los nodos actualizan sus características basándose en la información de sus vecinos\n",
        "\n",
        "2. En términos teóricos, ¿Qué diferencias genera instanciar la red con 0, 2 y 4 `hidden_layers`?\n",
        "\n",
        "R. En términos teóricos, al instanciar una red con 0 hidden_layers se está creando una red muy sencilla que proyecta las características de entrada a las de salida, sin realizar ninguna transformación o aprendizaje intermedio. Con 2 hidden_layers, la red tiene la oportunidad de aprender representaciones más complejas, ya que hay capas intermedias que pueden capturar características no lineales. Con 4 hidden_layers, la red es aún más profunda, y por lo tanto tiene una mayor capacidad para aprender representaciones más abstractas y complejas. Sin embargo, también corre un mayor riesgo de sobreajuste, ya que más capas pueden llevar a que el modelo memorice los datos de entrenamiento en lugar de aprender patrones generalizables.\n",
        "\n",
        "3. ¿Qué red tiene mejor accuracy? ¿A qué crees que se debe esta diferencia?\n",
        "\n",
        "R. La precisión de cada red dependerá de varios factores, incluyendo el número de épocas de entrenamiento, el tamaño de los batches, la tasa de aprendizaje, y otros hiperparámetros. En general, se podría esperar que una red con un número moderado de capas ocultas (por ejemplo, 2) tenga una mejor precisión que una red con 0 o muchas capas ocultas, porque tiene la capacidad de aprender representaciones más complejas sin caer en el sobreajuste. Sin embargo, esto también dependerá del conjunto de datos específico y la tarea de clasificación.\n",
        "\n",
        "4. Crea una nueva instancia de la red con un número de `hidden_layers` distinto al utilizado. Compara los resultados obtenidos y justifica por qué crees que son esos, considerando la cantidad de hidden layers que elegiste.\n"
      ],
      "metadata": {
        "id": "qWBiz5dhPBuC"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}